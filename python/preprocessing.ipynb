{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mne\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.colors as mcolors\n",
    "import math\n",
    "import numpy.ma as ma\n",
    "import statsmodels.formula.api as sm\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Correct Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../../') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Participant List \n",
    "- Selects data we want to load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_args = dict(gfp=True, time_unit='ms') # for styling plots for comparing evokeds (?)\n",
    "topomap_args = dict(sensors=False, time_unit='ms') # for styling topomaps (?)\n",
    "\n",
    "ID_list = ['P1', 'P3', 'P4', 'P5', 'P7', 'P8', 'P10', 'P11', 'P12', 'P13', 'P14', 'P15', 'P16',\n",
    "           'P17', 'P19', 'P20', 'P21', 'P22', 'P23', 'P24', 'P25', 'P26', 'P27', 'P28', 'P29', 'P30'] \n",
    "\n",
    "number_of_blocks = {'P1':20, 'P3':16, 'P4':20, 'P5':20, 'P7':20, 'P8':20, 'P10':20, 'P11':16,\n",
    "                    'P12':18,  'P13':16, 'P14':16, 'P15':17, 'P16':16, 'P17':17, 'P19':16, \n",
    "                    'P20':18, \"P21\": 20, \"P22\": 18, \"P23\": 20, \"P24\":18, \"P25\": 20, \"P26\": 20, \n",
    "                    'P27': 20, 'P28': 20, \"P29\": 20, \"P30\": 20}\n",
    "\n",
    "index = ID_list.index('P1') # select participant here\n",
    "ID = ID_list[index]\n",
    "num_blocks = number_of_blocks[ID]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Raw Data\n",
    "- Loads raw time-series data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read first block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1 # sets the block you want to load in\n",
    "raw_arrays = []\n",
    "fname = 'Harvey EEG Data/' + str(ID) + '/' + str(ID) + '_' + str(i) + '.bdf'\n",
    "raw = mne.io.read_raw_bdf(fname, preload = True) # loads in raw file\n",
    "raw_arrays.append(raw.get_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in mutiple files from one participant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (2, (num_blocks + 1)): # sets the rest of the blocks you want to load in\n",
    "    fname = 'Harvey EEG Data/' + str(ID) +'/'+ str(ID) + '_' + str(i) + '.bdf'\n",
    "    single_raw = mne.io.read_raw_bdf(fname, preload = True)\n",
    "    raw_arrays.append(single_raw.get_data())\n",
    "    raw = mne.io.concatenate_raws([raw, single_raw],verbose=False) # concatenates files together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detrend the Data to Remove Slow-Drifts\n",
    "\n",
    "1. Get raw data in array form (n_channels, n_times) \n",
    "2. Apply linear detrend to each channel using scipy.signal.detrend\n",
    "3. Put data back into mne object form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply detrending function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detrended_raw_arrays = []\n",
    "\n",
    "import scipy.signal\n",
    "    \n",
    "for i in range(0, len(raw_arrays)):\n",
    "    data = raw_arrays[i]\n",
    "    detrended_raw_array = np.zeros(data.shape)\n",
    "        \n",
    "    for j in range(0, raw_arrays[0].shape[0]): # iterate through all channels\n",
    "        detrended_raw_array[j] = scipy.signal.detrend(data[j], axis = 0, type = \"linear\", bp = 0)\n",
    "        \n",
    "    detrended_raw_arrays.append(detrended_raw_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put data back in MNE object form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detrended_raw_obj = []\n",
    "info = raw.info\n",
    "\n",
    "for i in range(0, len(detrended_raw_arrays)):\n",
    "    detrended_raw_obj.append(mne.io.RawArray(detrended_raw_arrays[i], info = info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low Pass Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_freq = 30 \n",
    "for i in range(0, len(detrended_raw_obj)):\n",
    "    detrended_raw_obj[i].filter(h_freq = h_freq, l_freq = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_for_events = raw\n",
    "raw =  mne.io.concatenate_raws(detrended_raw_obj, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redefine Channels\n",
    "- Channels 1-128 are defined as EEG (scalp channels)\n",
    "\n",
    "- Redefiining channel 129 and 130 as ECG channels\n",
    "\n",
    "- Redefining channels 131-144 as misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining channels 129 and 130 as EOG channels ##\n",
    "raw.set_channel_types({raw.ch_names[128]: 'ecg'}) \n",
    "raw.set_channel_types({raw.ch_names[129]: 'ecg'})\n",
    "\n",
    "## Defining channels 131-144 as miscellaneous ##\n",
    "for n in range(130,143):\n",
    "    raw.set_channel_types({raw.ch_names[n]: 'misc'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Bipolar EOG Channel\n",
    "- For removing blink artefacts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anode = 'EXG1' # computes anode minus cathode, so upper = anode, lower = cathode\n",
    "cathode = 'EXG2'\n",
    "ch_name = 'Bipolar_EOG' # setting name for resulting bipolar channel\n",
    "ch_info = None # can add dict with info about new channel\n",
    "drop_refs = False # if True, single external channels are dropped from data, False retains them\n",
    "copy = False # False modifies in place, True creates a copy and acts on that\n",
    "\n",
    "mne.set_bipolar_reference(raw, anode = anode, cathode = cathode, ch_name = ch_name, ch_info = ch_info, \n",
    "                          drop_refs = drop_refs, copy = copy)\n",
    "\n",
    "raw.set_channel_types({'Bipolar_EOG': 'eog'}) # Setting Bipolar_Eog as eog type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Montage\n",
    "Defining the montage as the 128 Biosemi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.set_montage('biosemi128')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolate Bad Channels\n",
    "- Lists the channels selected for interpolation for each participant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "if ID == 'P1':\n",
    "    raw.info['bads'] = [ 'C25', 'B28', 'C14', 'A20', 'A9', \"C16\", \"B29\"] \n",
    "    ch_names_to_skip = []\n",
    "elif ID == 'P3':\n",
    "    raw.info['bads'] = [ 'C25', 'A8', 'C3']\n",
    "    ch_names_to_skip = []\n",
    "elif ID == 'P4':\n",
    "    raw.info['bads'] = [ 'C25']\n",
    "    ch_names_to_skip = []\n",
    "elif ID == 'P5': # not included\n",
    "    raw.info['bads'] = ['C14', 'C25', 'A9', 'A16', 'B8', 'A10', 'A17', 'B10', 'B11', 'B7', 'A29']\n",
    "elif ID == 'P7':\n",
    "    raw.info['bads'] = [ 'C25', 'C29']\n",
    "    ch_names_to_skip = []\n",
    "elif ID == 'P8':\n",
    "    raw.info['bads'] = [ 'C14', 'C25']\n",
    "    ch_names_to_skip = []\n",
    "elif ID == 'P10':\n",
    "    raw.info['bads'] = [ 'C14', 'C25', 'A13', 'D4', 'D27']\n",
    "    ch_names_to_skip = ['B26', 'B14']\n",
    "elif ID == 'P11':\n",
    "    raw.info['bads'] = [ 'C14', 'C25']\n",
    "    ch_names_to_skip = []\n",
    "elif ID == 'P12':\n",
    "    raw.info['bads'] = [ 'C25', 'D9']\n",
    "    ch_names_to_skip = []\n",
    "elif ID == 'P13':\n",
    "    raw.info['bads'] = [ 'C25']\n",
    "    ch_names_to_skip = []\n",
    "elif ID == 'P14':\n",
    "    raw.info['bads'] = [ 'C25', 'C17'] # maybe not C17\n",
    "    ch_names_to_skip = []\n",
    "elif ID == 'P15':\n",
    "    raw.info['bads'] = [ 'C25', 'D26']\n",
    "    ch_names_to_skip = []\n",
    "elif ID == 'P16':\n",
    "    raw.info['bads'] = [ 'C25', 'D6', 'A12', 'B8', 'B6', 'B5']\n",
    "    ch_names_to_skip = []\n",
    "elif ID == 'P17':\n",
    "    raw.info['bads'] = ['C5', 'D4']\n",
    "    ch_names_to_skip = []\n",
    "elif ID == 'P19':\n",
    "    raw.info['bads'] = [ 'C25', 'B27', 'B28', 'C9', 'D8', 'D9', 'A11', 'B4', 'D4']\n",
    "    ch_names_to_skip = []\n",
    "elif ID == 'P20':\n",
    "    raw.info['bads'] = [ 'C25', 'B20', 'B2', 'D5', 'C30']\n",
    "    ch_names_to_skip = []\n",
    "elif ID == 'P21':\n",
    "    raw.info['bads'] = ['B7']\n",
    "    ch_names_to_skip = []\n",
    "elif ID == 'P22':\n",
    "    raw.info['bads'] = ['A32', 'C6', 'C7', 'D5', 'C8', 'D6', 'A9']\n",
    "    ch_names_to_skip = []\n",
    "elif ID == 'P23':\n",
    "    raw.info['bads'] = ['D11', 'C1', 'D7']\n",
    "    ch_names_to_skip = []\n",
    "elif ID == 'P24':\n",
    "    raw.info['bads'] = ['C3', 'C2', 'B29', 'C26', 'C21', 'C6', 'C7', 'B27']\n",
    "    ch_names_to_skip = []\n",
    "elif ID == 'P25':\n",
    "    raw.info['bads'] = ['D20', 'C22', 'D10', 'C3', 'B29', 'B3', 'B4', 'B32', 'B25']\n",
    "    ch_names_to_skip = []\n",
    "elif ID == 'P26':\n",
    "    raw.info['bads'] = ['A13']\n",
    "    ch_names_to_skip = []\n",
    "elif ID == 'P27':\n",
    "    raw.info['bads'] = []\n",
    "    ch_names_to_skip = []\n",
    "elif ID == 'P28':\n",
    "    raw.info['bads'] = [\"A32\", \"B22\", \"B29\"]\n",
    "    ch_names_to_skip = []\n",
    "elif ID == 'P29':\n",
    "    raw.info['bads'] = [\"D10\"]\n",
    "    ch_names_to_skip = []\n",
    "elif ID == 'P30':\n",
    "    raw.info['bads'] = []\n",
    "    ch_names_to_skip = []\n",
    "    \n",
    "raw.info['bads']\n",
    "raw.interpolate_bads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Average Reference\n",
    "- Re-referencing to the average of the 128 scalp electrodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.set_eeg_reference('average', projection = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Events\n",
    "- Finding all the events on the Status channel. \n",
    "\n",
    "- mne.find_events returns a n_events by 3 array describing the events \n",
    "\n",
    "    - column 1 indicates event time in samples\n",
    "    - column 2 indicates value of status channel immediately before the event\n",
    "    - column 3 indicates event ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_channel = 'Status' # Name of the status channel that tracks the triggers\n",
    "min_duration = 0.002 # 0 is MNE default # 0.002 # specifies the minimum duration of a change (in seconds) in stim channel that is required for it to be consideredan event\n",
    "initial_event = True # if True an event is created if the stim channel has a value different from 0 as its first sample\n",
    "\n",
    "events_detrended = mne.find_events(raw_for_events, stim_channel = stim_channel, min_duration = min_duration, initial_event = initial_event)\n",
    "raw.add_events(events_detrended, stim_channel = stim_channel, replace = True)\n",
    "min_duration = 0 # have to change this \n",
    "events = mne.find_events(raw, stim_channel = stim_channel, min_duration = min_duration, initial_event = initial_event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track Trial Numbers\n",
    "- This is only applicable to a small number of participants (P7) who have certain triggers missing on some trials\n",
    "\n",
    "- Need to know which trials don't have an encoding, end encode, reproduction or response trigger in them\n",
    "\n",
    "- So this cell marks down what the trigger values are in each trial by iterating through the events array, first checking if the trigger is an encode trigger, and then checks if the other triggers that should be there are indeed there\n",
    "\n",
    "- Trigger values from trials that do not contain all events are then removed from the events, so that only trials with all 4 encoding, end encode, repo and response triggers are included for the analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "num_trials = 45 * num_blocks # 5 mini blocks of 9 trials in each block = 45 trials\n",
    "\n",
    "trials = pd.DataFrame({\"Trial_Number\": np.arange(0,num_trials), \n",
    "                       \"Encode\": np.zeros(num_trials),\n",
    "                       \"Encode_n\": np.zeros(num_trials),\n",
    "                       \"SI\": np.zeros(num_trials),\n",
    "                       \"End_Encode\": np.zeros(num_trials),\n",
    "                       \"End_Encode_n\": np.zeros(num_trials),\n",
    "                       \"Repo\": np.zeros(num_trials),\n",
    "                       \"Repo_n\": np.zeros(num_trials),\n",
    "                       \"Response\": np.zeros(num_trials),\n",
    "                        \"Response_n\": np.zeros(num_trials),\n",
    "                        \"Gap_Time\": np.zeros(num_trials),\n",
    "                        \"Response_Time\": np.zeros(num_trials), \n",
    "                        \"Encode_Time\": np.zeros(num_trials)})\n",
    "\n",
    "## Storing Trigger Value and Location in Events Array on Each Trial (trig = 0 if no trigger on that trial) ##\n",
    "\n",
    "encode_triggers = [51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
    "end_triggers = [101, 102, 103, 104, 105, 106, 107, 108, 109]\n",
    "repo_triggers = [151, 152, 153, 154, 155, 156, 157, 158, 159]\n",
    "response_locked_triggers = [201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
    "\n",
    "index = 0\n",
    "for n in range(0, len(events) - 3): \n",
    "    set_index = 0\n",
    "    if events[n,2] in encode_triggers: # first checks if the trigger is an encode trigger \n",
    "        trials.at[index, \"Encode\"] = events[n,2] # marks the encode trigger value\n",
    "        trials.at[index, \"Encode_n\"] = n # marks the row value in the events array\n",
    "        if events[n,2] == 51: # adding 05/12/22\n",
    "            trials.at[index, \"SI\"] = 600\n",
    "        elif events[n, 2] == 52:\n",
    "            trials.at[index, \"SI\"] = 650\n",
    "        elif events[n, 2] == 53:\n",
    "            trials.at[index, \"SI\"] = 700\n",
    "        elif events[n, 2] == 54:\n",
    "            trials.at[index, \"SI\"] = 750\n",
    "        elif events[n, 2] == 55:\n",
    "            trials.at[index, \"SI\"] = 800\n",
    "        elif events[n, 2] == 56:\n",
    "            trials.at[index, \"SI\"] = 850\n",
    "        elif events[n, 2] == 57:\n",
    "            trials.at[index, \"SI\"] = 900\n",
    "        elif events[n, 2] == 58:\n",
    "            trials.at[index, \"SI\"] = 950\n",
    "        elif events[n, 2] == 59:\n",
    "            trials.at[index, \"SI\"] = 1000\n",
    "        \n",
    "        if events[n + 1, 2] in end_triggers:\n",
    "            trials.at[index, \"End_Encode\"] = events[n + 1, 2] # marks the trigger value\n",
    "            trials.at[index, \"End_Encode_n\"] = n + 1 # marks the row value in the events array\n",
    "            encode_samps = events[n + 1, 0] - events[n, 0] # 5/12/22\n",
    "            encode_ms = encode_samps / 512 * 1000 # converting to ms\n",
    "            trials.at[index, \"Encode_Time\"] = encode_ms\n",
    "            if events[n + 2, 2] in repo_triggers: # adding gap time to dataframe\n",
    "                gap_samps = events[n + 2, 0] - events[n + 1, 0] # repo trigger - end trigger\n",
    "                gap_ms = gap_samps / 512 * 1000 # converting to ms\n",
    "                trials.at[index, \"Gap_Time\"] = gap_ms\n",
    "        elif events[n + 1, 2] not in end_triggers:\n",
    "            trials.at[index, \"End_Encode\"] = 0 \n",
    "            trials.at[index, \"End_Encode_n\"] = 0\n",
    "            \n",
    "        if events[n + 2, 2] in repo_triggers:\n",
    "            trials.at[index, \"Repo\"] = events[n + 2, 2] # marks the trigger value\n",
    "            trials.at[index, \"Repo_n\"] = n + 2\n",
    "        elif events[n + 2, 2] not in repo_triggers:\n",
    "            trials.at[index, \"Repo\"] = 0\n",
    "            trials.at[index, \"Repo_n\"] = 0\n",
    "            \n",
    "        if events[n + 3, 2] in response_locked_triggers:\n",
    "            trials.at[index, \"Response\"] = events[n + 3, 2]\n",
    "            trials.at[index, \"Response_n\"] = n + 3\n",
    "            rt_samps = events[n + 3, 0] - events[n + 2, 0]\n",
    "            rt = rt_samps / 512 * 1000\n",
    "            trials.at[index, \"Response_Time\"] = rt\n",
    "        elif events[n + 3, 2] not in response_locked_triggers:\n",
    "            trials.at[index, \"Response\"] = 0\n",
    "            trials.at[index, \"Response_n\"] = 0\n",
    "        index += 1\n",
    "        \n",
    "    elif events[n, 2] not in encode_triggers: # if the trigger is not an encode trigger, it checks the subsequent triggers to see if the other 3 line up but the encode is missing\n",
    "        trials.at[index, \"Encode\"] = 0\n",
    "        trials.at[index, \"Encode_n\"] = 0\n",
    "        if events[n + 1, 2] in end_triggers:\n",
    "            trials.at[index, \"End_Encode\"] = events[n + 1, 2]\n",
    "            trials.at[index, \"End_Encode_n\"] = n + 1\n",
    "            set_index = 1\n",
    "        if events[n + 2, 2] in repo_triggers:\n",
    "            trials.at[index, \"Repo\"] = events[n + 2, 2]\n",
    "            trials.at[index, \"Repo_n\"] = n + 2\n",
    "            set_index = 1\n",
    "        if events[n + 3, 2] in response_locked_triggers:\n",
    "            trials.at[index, \"Response\"] = events[n + 3, 2]\n",
    "            trials.at[index, \"Response_n\"] = n + 3\n",
    "            set_index = 1\n",
    "        if set_index == 1:\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Trigger Values and Trial Numbers of Trials With Missing Triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store the Row Value of the Valid Triggers in the Events Array For Trials in Which another trigger is missing #\n",
    "## This means these triggers can be removed from the Events array so that only trials with all 4 triggers remain \n",
    "\n",
    "trials_with_no_encode_triggers = []\n",
    "trials_with_no_gap_triggers = []\n",
    "trials_with_no_repo_triggers = []\n",
    "trials_with_no_response_triggers = []\n",
    "\n",
    "rows_to_delete = []\n",
    "\n",
    "for i in range(0, len(trials)):\n",
    "    if trials.at[i, \"Encode\"] == 0: # Trials with no Encode\n",
    "        if i not in rows_to_delete:\n",
    "            rows_to_delete.append(i)\n",
    "        if trials.at[i, \"End_Encode\"] != 0:\n",
    "            trials_with_no_encode_triggers.append(trials.at[i, \"End_Encode_n\"])\n",
    "        if trials.at[i, \"Repo\"] != 0:\n",
    "            trials_with_no_encode_triggers.append(trials.at[i, \"Repo_n\"])\n",
    "        if trials.at[i, \"Response\"] != 0:\n",
    "            trials_with_no_encode_triggers.append(trials.at[i, \"Response_n\"])\n",
    "            \n",
    "    if trials.at[i, \"End_Encode\"] == 0: ## Trials with no gap\n",
    "        if i not in rows_to_delete:\n",
    "            rows_to_delete.append(i)\n",
    "        if trials.at[i, \"Encode\"] != 0:\n",
    "            trials_with_no_gap_triggers.append(trials.at[i, \"Encode_n\"])\n",
    "        if trials.at[i, \"Repo\"] != 0:\n",
    "            trials_with_no_gap_triggers.append(trials.at[i, \"Repo_n\"])\n",
    "        if trials.at[i, \"Response\"] != 0:\n",
    "            trials_with_no_gap_triggers.append(trials.at[i, \"Response_n\"])\n",
    "            \n",
    "    if trials.at[i, \"Repo\"] == 0: ## Trials with no Repo\n",
    "        if i not in rows_to_delete:\n",
    "            rows_to_delete.append(i)\n",
    "        if trials.at[i, \"Encode\"] != 0:\n",
    "            trials_with_no_repo_triggers.append(trials.at[i, \"Encode_n\"])\n",
    "        if trials.at[i, \"End_Encode\"] != 0:\n",
    "            trials_with_no_repo_triggers.append(trials.at[i, \"End_Encode_n\"])\n",
    "        if trials.at[i, \"Response\"] != 0:\n",
    "            trials_with_no_repo_triggers.append(trials.at[i, \"Response_n\"])\n",
    "        \n",
    "    if trials.at[i, \"Response\"] == 0: ## Trials with no Response\n",
    "        if i not in rows_to_delete:\n",
    "            rows_to_delete.append(i)\n",
    "        if trials.at[i, \"Encode\"] != 0:\n",
    "            trials_with_no_response_triggers.append(trials.at[i, \"Encode_n\"])\n",
    "        if trials.at[i, \"End_Encode\"] != 0:\n",
    "            trials_with_no_response_triggers.append(trials.at[i, \"End_Encode_n\"])\n",
    "        if trials.at[i, \"Repo\"] != 0:\n",
    "            trials_with_no_response_triggers.append(trials.at[i, \"Repo_n\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete Missing Events \n",
    "- The below cell removes any events that are in trials that do not contain all 4 events, e.g removes encoding triggers that have no corresponding end encode, repo or response triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triggers_for_deletion = np.unique(np.array(trials_with_no_encode_triggers + trials_with_no_gap_triggers + trials_with_no_repo_triggers + trials_with_no_response_triggers, dtype = \"int\"))\n",
    "\n",
    "if len(triggers_for_deletion) > 0:\n",
    "    events = np.delete(arr = events, obj = triggers_for_deletion, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials.drop(rows_to_delete, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Test Behavioural Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "av_response_time = np.zeros(9) # output array for mean rt for each sample interval condition\n",
    "median_response_time = np.zeros(9) # # output array for median rt for each sample interval condition\n",
    "\n",
    "fig, axs = plt.subplots(1,9,figsize=(15,4),sharey=True) # creating axes for big plot below\n",
    "\n",
    "sample_interval_durations = np.arange(600,1001,50) # (600, 650, 700, 750, 800, 850, 900, 950, 1000)\n",
    "\n",
    "for i in range(0, len(sample_interval_durations)):\n",
    "    av_response_time[i] = trials[\"Response_Time\"][trials[\"Encode\"] == encode_triggers[i]].mean()\n",
    "    median_response_time[i] = trials[\"Response_Time\"][trials[\"Encode\"] == encode_triggers[i]].median()\n",
    "    axs[i].hist(trials[\"Response_Time\"][trials[\"Encode\"] == encode_triggers[i]])\n",
    "    axs[i].axvline(np.average(trials[\"Response_Time\"][trials[\"Encode\"] == encode_triggers[i]]),color='k',linewidth=4, alpha = 0.7) # vertical line for mean\n",
    "    axs[i].axvline(np.median(trials[\"Response_Time\"][trials[\"Encode\"] == encode_triggers[i]]),color='g',linewidth=4, alpha = 0.7) # vertical line for median\n",
    "    axs[i].set_title(sample_interval_durations[i]) # sets title name for individual histogram\n",
    "    axs[i].set_xlim([500,1500])\n",
    "    axs[i].set_xlabel('Response \\n Times (ms)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(trials[\"SI\"], trials[\"Response_Time\"])\n",
    "\n",
    "## Plotting ALL Sample Interval/Encoding Time Versus ALL Response Times ##\n",
    "\n",
    "plt.subplots(1,1,figsize=(5,5))\n",
    "\n",
    "plt.plot(trials[\"SI\"], trials[\"Response_Time\"],'.', alpha = 0.2) # plots response time in each trial against sample interval in each trial\n",
    "plt.plot(sample_interval_durations, av_response_time,':ob', label = 'Average Reproduction', alpha = 1) # plots average response time for each condition\n",
    "plt.plot(sample_interval_durations, median_response_time,':og', label = 'Median Reproduction', alpha = 1) # plots median response time for each condition \n",
    "\n",
    "plt.plot(sample_interval_durations,sample_interval_durations,'-.k',label = 'Veridical Encoder') # Plots what a veridical processor would reproduce (m = 1)\n",
    "plt.plot(sample_interval_durations,800*np.ones(9),'--k',label = 'Average Encoder') # Plots what a perfectly average processor would reproduce (m = 0)\n",
    "plt.xlabel('Sample Interval (ms)')\n",
    "plt.ylabel('Response Time (ms)')\n",
    "plt_title = str(ID) + ' Response Times'\n",
    "\n",
    "plt.title(plt_title)\n",
    "plt.ylim((0,1500))\n",
    "#plt.legend()\n",
    "if p_value <0.05:\n",
    "    plt.text(850,300,r'$Slope$='+ str(np.round(slope,2)) + r'$^*$')\n",
    "else:\n",
    "    plt.text(850,300,r'$Slope$='+ str(np.round(slope,2)) )\n",
    "\n",
    "plt.text(850,200,r'$Intercept$='+ str(np.round(intercept,2)))\n",
    "plt.text(850,100,r'$R^2$=' + str(np.round(r_value**2,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Event Dictionary\n",
    "\n",
    "## Encoding Events\n",
    "|Trigger|| Event|\n",
    "|-------||------|\n",
    "| 51    || Encode 600ms|\n",
    "| 52    || Encode 650ms|\n",
    "| 53    || Encode 700ms|\n",
    "| 54    || Encode 750ms|\n",
    "| 55    || Encode 800ms|\n",
    "| 56    || Encode 850ms|\n",
    "| 57    || Encode 900ms|\n",
    "| 58    || Encode 950ms|\n",
    "| 59    || Encode 1000ms|\n",
    "\n",
    "\n",
    "\n",
    "## End of Encoding Events\n",
    "|Trigger|| Event|\n",
    "|-------||------|\n",
    "| 101    || End Encode 600ms|\n",
    "| 102    || End Encode 650ms|\n",
    "| 103    || End Encode 700ms|\n",
    "| 104    || End Encode 750ms|\n",
    "| 105    || End Encode 800ms|\n",
    "| 106    || End Encode 850ms|\n",
    "| 107    || End Encode 900ms|\n",
    "| 108    || End Encode 950ms|\n",
    "| 109    || End Encode 1000ms|\n",
    "\n",
    "\n",
    "\n",
    "## Start of Reproduction \n",
    "|Trigger|| Event|\n",
    "|-------||------|\n",
    "| 151    || Start 600ms|\n",
    "| 152    || Start 650ms|\n",
    "| 153    || Start 700ms|\n",
    "| 154    || Start 750ms|\n",
    "| 155    || Start 800ms|\n",
    "| 156    || Start 850ms|\n",
    "| 157    || Start 900ms|\n",
    "| 158    || Start 950ms|\n",
    "| 159    || Start 1000ms|\n",
    "\n",
    "\n",
    "## Response\n",
    "|Trigger|| Event|\n",
    "|-------||------|\n",
    "| 201    || Response 600ms|\n",
    "| 202    || Response 650ms|\n",
    "| 203    || Response 700ms|\n",
    "| 204    || Response 750ms|\n",
    "| 205    || Response 800ms|\n",
    "| 206    || Response 850ms|\n",
    "| 207    || Response 900ms|\n",
    "| 208    || Response 950ms|\n",
    "| 209    || Response 1000ms|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epoch Data into 3 Segments for Analysis\n",
    "- Encoding epoch = pre-interval to post-interval\n",
    "\n",
    "- Reproduction epoch = pre-cue (pre start of reproduction phase) - post-cue\n",
    "\n",
    "- Response-Locked epoch = pre-response - post response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Baseline Period\n",
    "- Can select to baseline each epoch to its own pre-epoch period (baseline = 0)\n",
    "\n",
    "- Can select to baseline each epoch to the pre-encoding phase interval (baseline = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_setting = 1\n",
    "\n",
    "if baseline_setting == 1:\n",
    "    baseline_name = \"pre_interval_baseline\"\n",
    "else:\n",
    "    baseline_name = \"own_baseline\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Encoding Epochs\n",
    "- Epochs are 1200ms duration, locked to the start of the encoding epoch (indicated by sample interval onset), using 200ms pre-stimulus period as baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_event_dict = {'Encode/600':51,'Encode/650':52,'Encode/700':53,'Encode/750':54,'Encode/800':55,\n",
    "                     'Encode/850':56,'Encode/900':57,'Encode/950':58,'Encode/1000':59}\n",
    "\n",
    "encode_epochs = mne.Epochs(raw, events, event_id = encode_event_dict, preload = True)#, tmin=-0.1, \n",
    "                    #tmax=1.1,reject=reject_criteria, preload=True,baseline=(-.1,0),verbose=False);\n",
    "  \n",
    "reject_criteria = dict(eeg = 100e-6, eog = 200e-6)  # 100 μV for scalp, # 200 μV for bipolar,\n",
    "\n",
    "# Seeing how many epochs in each condition\n",
    "original_num_epochs = np.zeros(9)\n",
    "\n",
    "for i in range(0, 9):\n",
    "    name = \"Encode/\" + str(sample_interval_durations[i])\n",
    "    encode = encode_epochs[name]\n",
    "    original_num_epochs[i] = len(encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add RT related Metadata to the Encoding Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt=events[encode_epochs.selection+3]-events[encode_epochs.selection+2]\n",
    "rt=rt[:,0]*1000/512\n",
    "mean_rt=np.zeros(len(rt))\n",
    "median_rt=np.zeros(len(rt))\n",
    "\n",
    "gaps = events[encode_epochs.selection+2]-events[encode_epochs.selection+1]\n",
    "gaps=gaps[:,0]*1000/512\n",
    "\n",
    "RT_gap = rt + gaps\n",
    "\n",
    "for j in range(0,len(av_response_time)):\n",
    "    mean_rt[encode_epochs.events[:,2]==51+j]=av_response_time[j]+50\n",
    "    median_rt[encode_epochs.events[:,2]==51+j]=median_response_time[j]+50\n",
    "\n",
    "Over_Under=np.zeros(len(rt))\n",
    "Over_Under[rt>median_rt]=1\n",
    "Trigger=encode_epochs.events[:,2]\n",
    "encoding_metadata={'trial_number':range(len(encode_epochs.events)),\n",
    "         'Response_Time':rt,\n",
    "         'Over_Under':Over_Under,\n",
    "         'Mean_RT':mean_rt,\n",
    "         'Median_RT':median_rt,\n",
    "         'Trigger':Trigger,\n",
    "         'Gap': gaps,\n",
    "         'Reject_Artif': np.zeros(len(encode_epochs.events)), \n",
    "         'RT_gap': RT_gap}\n",
    "encoding_metadata=pd.DataFrame(encoding_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Encoding Epochs Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_epoch_events = encode_epochs.events \n",
    "event_id = encode_event_dict\n",
    "preload = False\n",
    "verbose = False\n",
    "metadata = encoding_metadata \n",
    "detrend = None # \n",
    "\n",
    "if baseline_setting == 0:\n",
    "    tmin = -0.2 \n",
    "    tmax = 1.2 \n",
    "    reject = None\n",
    "    baseline = (-0.2, 0) # should apply baseline, from mne: \"Correction is applied to each epoch and channel individually in the following way: Calculate the mean signal of the baseline period. Subtract this mean from the entire epoch.\"\n",
    "\n",
    "elif baseline_setting == 1:\n",
    "    tmin = -0.2\n",
    "    tmax = 1.2 \n",
    "    reject = None\n",
    "    baseline = (-0.2, 0) \n",
    "\n",
    "encode_epochs = mne.Epochs(raw, encoding_epoch_events, event_id = encode_event_dict, tmin = tmin, tmax = tmax, \n",
    "                           reject = reject, preload = preload, detrend = detrend, baseline = baseline, \n",
    "                           verbose = verbose, metadata = metadata)\n",
    "\n",
    "encode_epochs.load_data()\n",
    "encode_epochs_copy = encode_epochs.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Baseline Epochs \n",
    "- Same as above but no baseline correction applied and only includes the baseline period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_epoch_events = encode_epochs.events \n",
    "event_id = encode_event_dict\n",
    "preload = False\n",
    "verbose = False \n",
    "metadata = encoding_metadata \n",
    "detrend = None #1 is linear detrend\n",
    "\n",
    "tmin = tmin\n",
    "tmax = 0\n",
    "reject = None\n",
    "baseline = None # not applying baseline correction so I can take out baseline period\n",
    "\n",
    "baseline_epochs = mne.Epochs(raw, encoding_epoch_events, event_id = encode_event_dict, tmin = tmin, tmax = tmax, \n",
    "                           reject = reject, preload = preload, detrend = detrend, baseline = baseline, \n",
    "                           verbose = verbose, metadata = metadata)\n",
    "baseline_epochs.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Across All Time Points in the Baseline Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_epochs_data = baseline_epochs.get_data()\n",
    "baseline_period = np.mean(baseline_epochs_data, axis = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Rejection Criteria To Encoding Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_epochs_data = encode_epochs.get_data()\n",
    "n_epochs, n_channels, n_times = encode_epochs_data.shape\n",
    "rejected_epochs = []\n",
    "\n",
    "ch_names = raw.ch_names\n",
    "ch_dict = {}\n",
    "\n",
    "for ch in ch_names[0:128]:\n",
    "    ch_dict[ch] = 0\n",
    "\n",
    "ch_dict[ch_names[-1]] = 0\n",
    "    \n",
    "ch_keys = list(ch_dict.keys())\n",
    "    \n",
    "for i in range(0, n_epochs):\n",
    "    single_epoch_data = encode_epochs_data[i, :, :]\n",
    "    eeg_channels = single_epoch_data[0:128, :]\n",
    "    bipolar_channel = single_epoch_data[-1, :]\n",
    "\n",
    "    if np.sum((abs(eeg_channels) > 100e-6)) >= 1: # removing epochs with eeg channel amplitude greater than 100e-6\n",
    "        rejected_epochs.append(i)\n",
    "        \n",
    "    elif np.sum((abs(bipolar_channel) > 200e-6)) >= 1: # removing epochs with bipolar channel amplitude greater than 200e-6\n",
    "        rejected_epochs.append(i)\n",
    "        ch_dict[ch_keys[-1]] += 1\n",
    "        if i not in rejected_epochs: ## added 05/12/22\n",
    "                rejected_epochs.append(i)\n",
    "        \n",
    "    for j in range(0, 128):\n",
    "        single_channel_data = eeg_channels[j, :]\n",
    "        if np.sum(abs(single_channel_data) > 100e-6) >= 1:\n",
    "            ch_dict[ch_keys[j]] += 1\n",
    "        \n",
    "encoding_rejected = rejected_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot number of trials excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove channels with no epochs rejected ##\n",
    "ch_dict_new = {}\n",
    "\n",
    "for key in ch_dict.keys():\n",
    "    if ch_dict[key] != 0:\n",
    "        ch_dict_new[key] = ch_dict[key]\n",
    "\n",
    "x = np.arange(0, len(ch_dict_new))\n",
    "height = (np.array(list(ch_dict_new.values())) / n_epochs) * 100\n",
    "tick_label = list(ch_dict_new.keys())\n",
    "\n",
    "percent_rejected = (len(rejected_epochs) / n_epochs) * 100\n",
    "\n",
    "plt.bar(x, height, color = \"gray\", alpha = 0.5)\n",
    "plt.title(ID + \" Encode Drop Log,\" + \" Total Rejected = \" + str(np.round(percent_rejected, 2)) + \"%\")\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tick_label)\n",
    "ax.tick_params(axis = 'x', labelrotation = 45)\n",
    "ax.set_ylabel(\"% Rejected\")\n",
    "plt.grid(axis = 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Rejected Epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_epochs.drop(np.array(rejected_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Artifact Rejection To Baseline Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_epochs_for_rejection = encode_epochs_copy.crop(encode_epochs_copy.tmin, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_rejection_data = baseline_epochs_for_rejection.get_data()\n",
    "n_epochs, n_channels, n_times = baseline_rejection_data.shape\n",
    "rejected_epochs = []\n",
    "\n",
    "ch_names = raw.ch_names\n",
    "ch_dict = {}\n",
    "\n",
    "for ch in ch_names[0:128]:\n",
    "    ch_dict[ch] = 0\n",
    "\n",
    "ch_dict[ch_names[-1]] = 0\n",
    "    \n",
    "ch_keys = list(ch_dict.keys())\n",
    "\n",
    "for i in range(0, n_epochs):\n",
    "    single_epoch_data = baseline_rejection_data[i, :, :]\n",
    "    eeg_channels = single_epoch_data[0:128, :]\n",
    "    bipolar_channel = single_epoch_data[-1, :]\n",
    "\n",
    "    if np.sum((abs(eeg_channels) > 100e-6)) >= 1: # removing epochs with eeg channel amplitude greater than 100e-6\n",
    "        rejected_epochs.append(i)\n",
    "        \n",
    "    elif np.sum((abs(bipolar_channel) > 200e-6)) >= 1: # removing epochs with bipolar channel amplitude greater than 200e-6\n",
    "        ch_dict[ch_keys[-1]] += 1\n",
    "        if i not in rejected_epochs:\n",
    "            rejected_epochs.append(i)\n",
    "        \n",
    "    for j in range(0, 128):\n",
    "        single_channel_data = eeg_channels[j, :]\n",
    "        if np.sum(abs(single_channel_data) > 100e-6) >= 1:\n",
    "            ch_dict[ch_keys[j]] += 1\n",
    "        \n",
    "baseline_rejected_epochs = rejected_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot number of trials excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove channels with no epochs rejected ##\n",
    "ch_dict_new = {}\n",
    "\n",
    "for key in ch_dict.keys():\n",
    "    if ch_dict[key] != 0:\n",
    "        ch_dict_new[key] = ch_dict[key]\n",
    "\n",
    "x = np.arange(0, len(ch_dict_new))\n",
    "height = (np.array(list(ch_dict_new.values())) / n_epochs) * 100\n",
    "tick_label = list(ch_dict_new.keys())\n",
    "\n",
    "percent_rejected = (len(rejected_epochs) / n_epochs) * 100\n",
    "\n",
    "plt.bar(x, height, color = \"gray\", alpha = 0.5)\n",
    "plt.title(ID + \" Baseline Drop Log,\" + \" Total Rejected = \" + str(np.round(percent_rejected, 2)) + \"%\")\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tick_label)\n",
    "ax.tick_params(axis = 'x', labelrotation = 45)\n",
    "ax.set_ylabel(\"% Rejected\")\n",
    "plt.grid(axis = 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Reproduction Epochs\n",
    "- Epochs are 1200ms duration, locked to the start of the reproduction epoch (indicated by cue?), using 100ms pre-stimulus period as baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_event_dict = {'Reproduction/600':151,'Reproduction/650':152,'Reproduction/700':153,'Reproduction/750':154,\n",
    "                   'Reproduction/800':155, 'Reproduction/850':156,'Reproduction/900':157,'Reproduction/950':158,\n",
    "                   'Reproduction/1000':159}\n",
    "\n",
    "repo_epochs = mne.Epochs(raw, events, event_id = repo_event_dict, preload = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add RT related Metadata to the Reproduction Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "rt = events[repo_epochs.selection + 1] - events[repo_epochs.selection]\n",
    "rt = rt[:,0] * 1000/512\n",
    "mean_rt = np.zeros(len(rt))\n",
    "median_rt = np.zeros(len(rt))\n",
    "\n",
    "gaps = events[repo_epochs.selection]-events[repo_epochs.selection-1]\n",
    "gaps=gaps[:,0]*1000/512\n",
    "\n",
    "RT_gap = rt + gaps\n",
    "\n",
    "for j in range(0,len(av_response_time)):\n",
    "    mean_rt[repo_epochs.events[:,2]==151+j]=av_response_time[j]+50\n",
    "    median_rt[repo_epochs.events[:,2]==151+j]=median_response_time[j]+50\n",
    "\n",
    "Over_Under=np.zeros(len(rt))\n",
    "Over_Under[rt>median_rt]=1\n",
    "Trigger=repo_epochs.events[:,2]\n",
    "repo_metadata={'trial_number':range(len(repo_epochs.events)),\n",
    "         'Response_Time':rt,\n",
    "         'Over_Under':Over_Under,\n",
    "         'Mean_RT':mean_rt,\n",
    "         'Median_RT':median_rt,\n",
    "         'Trigger':Trigger,\n",
    "         'Gap': gaps,\n",
    "         'Reject_Artif': np.zeros(len(repo_epochs.events)),\n",
    "         'RT_gap': RT_gap}\n",
    "repo_metadata = pd.DataFrame(repo_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Reproduction Epochs Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reproduction_events = repo_epochs.events \n",
    "event_id = repo_event_dict\n",
    "preload = False\n",
    "verbose = False \n",
    "detrend = None # 1 is linear detrend\n",
    "metadata = repo_metadata \n",
    "\n",
    "if baseline_setting == 0: # not using this version currently\n",
    "    tmin = -0.2 \n",
    "    tmax = 1.2 \n",
    "    baseline = (-0.2, 0) # baselined to the average of activity from -200ms to 0ms from start of epoch\n",
    "    reject = None \n",
    "\n",
    "elif baseline_setting == 1:\n",
    "    tmin = -0.3 # changed 16/12/22\n",
    "    tmax = 0.9 # changed 16/12/22\n",
    "    baseline = None \n",
    "    reject = None \n",
    "    \n",
    "repo_epochs = mne.Epochs(raw, reproduction_events, event_id = repo_event_dict, tmin = tmin, tmax = tmax, \n",
    "                         reject = reject, preload = preload, verbose = False, detrend = detrend,\n",
    "                         metadata = metadata, baseline = baseline)\n",
    "\n",
    "repo_epochs.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Epochs\n",
    "Baselining reproduction epochs by subtracting pre-interval baseline period from all times in reproduction epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if baseline_setting == 1:\n",
    "    repo_epochs_data = repo_epochs.get_data()\n",
    "    repo_epochs_data_baselined = np.zeros(repo_epochs_data.shape)\n",
    "    \n",
    "    for i in range(0, n_epochs):\n",
    "        for j in range(0, n_channels):\n",
    "            baseline = baseline_period[i, j]\n",
    "            repo_epochs_data_baselined[i, j, :] = repo_epochs_data[i, j, :] - baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreate Epochs Object\n",
    "Have to do this again with new data if baselining to pre-interval period\n",
    "\n",
    "Doing it before artifact rejection because I have to .drop the rejects epochs from an epochs object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if baseline_setting == 1:\n",
    "    \n",
    "    data = repo_epochs_data_baselined\n",
    "    reproduction_events = repo_epochs.events \n",
    "    event_id = repo_event_dict\n",
    "    tmin = repo_epochs.tmin \n",
    "    reject = None\n",
    "    baseline = None\n",
    "    info = repo_epochs.info\n",
    "    \n",
    "    repo_epochs = mne.EpochsArray(data = data, info = info, events = reproduction_events, event_id = event_id, \n",
    "                                  tmin = tmin, reject = reject, baseline = baseline, metadata = metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Artifact Rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_epochs_data = repo_epochs.get_data()\n",
    "n_epochs, n_channels, n_times = repo_epochs_data.shape\n",
    "rejected_epochs = []\n",
    "\n",
    "ch_names = raw.ch_names\n",
    "ch_dict = {}\n",
    "\n",
    "for ch in ch_names[0:128]:\n",
    "    ch_dict[ch] = 0\n",
    "\n",
    "ch_dict[ch_names[-1]] = 0\n",
    "    \n",
    "ch_keys = list(ch_dict.keys())\n",
    "\n",
    "for i in range(0, n_epochs):\n",
    "    single_epoch_data = repo_epochs_data[i, :, :]\n",
    "    eeg_channels = single_epoch_data[0:128, :]\n",
    "    bipolar_channel = single_epoch_data[-1, :]\n",
    "\n",
    "    if np.sum((abs(eeg_channels) > 100e-6)) >= 1: # removing epochs with eeg channel amplitude greater than 100e-6\n",
    "        rejected_epochs.append(i)\n",
    "        \n",
    "    elif np.sum((abs(bipolar_channel) > 200e-6)) >= 1: # removing epochs with bipolar channel amplitude greater than 200e-6\n",
    "        ch_dict[ch_keys[-1]] += 1\n",
    "        if i not in rejected_epochs:\n",
    "            rejected_epochs.append(i)\n",
    "        \n",
    "    for j in range(0, 128):\n",
    "        single_channel_data = eeg_channels[j, :]\n",
    "        if np.sum(abs(single_channel_data) > 100e-6) >= 1:\n",
    "            ch_dict[ch_keys[j]] += 1\n",
    "\n",
    "repo_rejected_epochs = rejected_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot number of trials excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove channels with no epochs rejected ##\n",
    "ch_dict_new = {}\n",
    "\n",
    "for key in ch_dict.keys():\n",
    "    if ch_dict[key] != 0:\n",
    "        ch_dict_new[key] = ch_dict[key]\n",
    "\n",
    "x = np.arange(0, len(ch_dict_new))\n",
    "height = (np.array(list(ch_dict_new.values())) / n_epochs) * 100\n",
    "tick_label = list(ch_dict_new.keys())\n",
    "\n",
    "percent_rejected = (len(rejected_epochs) / n_epochs) * 100\n",
    "\n",
    "plt.bar(x, height, color = \"gray\", alpha = 0.5)\n",
    "plt.title(ID + \" Reproduction Drop Log,\" + \" Total Rejected = \" + str(np.round(percent_rejected, 2)) + \"%\")\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tick_label)\n",
    "ax.tick_params(axis = 'x', labelrotation = 45)\n",
    "ax.set_ylabel(\"% Rejected\")\n",
    "plt.grid(axis = 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add trial numbers of baseline epochs that were rejected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in baseline_rejected_epochs:\n",
    "    if i not in rejected_epochs:\n",
    "        rejected_epochs.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Rejected Epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_epochs.drop(np.array(rejected_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Response Locked Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_event_dict = {'Response/600':201,'Response/650':202,'Response/700':203,'Response/750':204,'Response/800':205,\n",
    "              'Response/850':206,'Response/900':207,'Response/950':208,'Response/1000':209}\n",
    "\n",
    "response_epochs = mne.Epochs(raw, events, event_id = response_event_dict, preload = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add RT related metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "rt = events[response_epochs.selection-1] - events[response_epochs.selection]\n",
    "rt=-rt[:,0]*1000/512\n",
    "plt.hist(rt)\n",
    "mean_rt=np.zeros(len(rt))\n",
    "median_rt=np.zeros(len(rt))\n",
    "\n",
    "gaps = events[response_epochs.selection-1]-events[response_epochs.selection-2]\n",
    "gaps=gaps[:,0]*1000/512\n",
    "\n",
    "RT_gap = rt + gaps\n",
    "\n",
    "for j in range(0,len(av_response_time)):\n",
    "    mean_rt[response_epochs.events[:,2]==201+j]=av_response_time[j]+50\n",
    "    median_rt[response_epochs.events[:,2]==201+j]=median_response_time[j]+50\n",
    "\n",
    "Over_Under=np.zeros(len(rt))\n",
    "Over_Under[rt>=median_rt]=1\n",
    "Trigger=response_epochs.events[:,2]\n",
    "response_locked_metadata={'trial_number':range(len(response_epochs.events)),\n",
    "         'Response_Time':rt,\n",
    "         'Over_Under':Over_Under,\n",
    "         'Mean_RT':mean_rt,\n",
    "         'Median_RT':median_rt,\n",
    "         'Trigger':Trigger,\n",
    "         'Gap': gaps,\n",
    "         'Reject_Artif': np.zeros(len(response_epochs.events)),\n",
    "         'RT_gap': RT_gap}\n",
    "response_locked_metadata=pd.DataFrame(response_locked_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Response Locked Epochs Object\n",
    "Creating epochs locked to the response in the response epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_locked_events = response_epochs.events \n",
    "event_id = response_event_dict \n",
    "metadata = response_locked_metadata \n",
    "detrend = None # 1 is linear detrend\n",
    "preload = False\n",
    "verbose = False\n",
    "\n",
    "if baseline_setting == 0: # not using this version currently\n",
    "    tmin = -1.2\n",
    "    tmax = 0.2 \n",
    "    reject = None\n",
    "    baseline = (-1.2, -1.0) # epochs baselined to averaged of activity between -1200ms and -1000ms\n",
    "\n",
    "if baseline_setting == 1:\n",
    "    tmin = -1.1 # epochs are -1200ms from response\n",
    "    tmax = 0.1 # epochs are 200ms from response\n",
    "    reject = None\n",
    "    baseline = None # will be baselined to pre-interval period after\n",
    "    \n",
    "response_locked_epochs = mne.Epochs(raw, events = response_locked_events, event_id = event_id, tmin = tmin, tmax = tmax,\n",
    "                             reject = reject, metadata = metadata, preload = preload, detrend = detrend, verbose = verbose,\n",
    "                             baseline = baseline)\n",
    "\n",
    "response_locked_epochs.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if baseline_setting == 1:\n",
    "    response_locked_epochs_data = response_locked_epochs.get_data()\n",
    "    response_locked_epochs_data_baselined = np.zeros(response_locked_epochs_data.shape)\n",
    "    \n",
    "    for i in range(0, n_epochs):\n",
    "        for j in range(0, n_channels):\n",
    "            baseline = baseline_period[i, j]\n",
    "            response_locked_epochs_data_baselined[i, j, :] = response_locked_epochs_data[i, j, :] - baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreate Epochs Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if baseline_setting == 1:\n",
    "    \n",
    "    data = response_locked_epochs_data_baselined\n",
    "    response_locked_events = response_epochs.events \n",
    "    event_id = response_event_dict \n",
    "    tmin = response_locked_epochs.tmin # sets the times attribute to -1.2 start point rather than starting at 0\n",
    "    reject = None\n",
    "    baseline = None\n",
    "    info = response_locked_epochs.info\n",
    "    \n",
    "    response_locked_epochs = mne.EpochsArray(data = data, info = info, events = response_locked_events, event_id = event_id, \n",
    "                                  tmin = tmin, reject = reject, baseline = baseline, metadata = metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Artifact Rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_locked_epochs_data = response_locked_epochs.get_data()\n",
    "n_epochs, n_channels, n_times = response_locked_epochs_data.shape\n",
    "rejected_epochs = []\n",
    "\n",
    "ch_names = raw.ch_names\n",
    "ch_dict = {}\n",
    "\n",
    "for ch in ch_names[0:128]:\n",
    "    ch_dict[ch] = 0\n",
    "\n",
    "ch_dict[ch_names[-1]] = 0\n",
    "    \n",
    "ch_keys = list(ch_dict.keys())\n",
    "\n",
    "for i in range(0, n_epochs):\n",
    "    single_epoch_data = response_locked_epochs_data[i, :, :]\n",
    "    eeg_channels = single_epoch_data[0:128, :]\n",
    "    bipolar_channel = single_epoch_data[-1, :]\n",
    "\n",
    "    if np.sum((abs(eeg_channels) > 100e-6)) >= 1: # removing epochs with eeg channel amplitude greater than 100e-6\n",
    "        rejected_epochs.append(i)\n",
    "        \n",
    "    elif np.sum((abs(bipolar_channel) > 200e-6)) >= 1: # removing epochs with bipolar channel amplitude greater than 200e-6\n",
    "        ch_dict[ch_keys[-1]] += 1\n",
    "        if i not in rejected_epochs:\n",
    "            rejected_epochs.append(i)\n",
    "        \n",
    "    for j in range(0, 128):\n",
    "        single_channel_data = eeg_channels[j, :]\n",
    "        if np.sum(abs(single_channel_data) > 100e-6) >= 1:\n",
    "            ch_dict[ch_keys[j]] += 1\n",
    "    \n",
    "response_locked_rejected_epochs = rejected_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot number of trials excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove channels with no epochs rejected ##\n",
    "ch_dict_new = {}\n",
    "\n",
    "for key in ch_dict.keys():\n",
    "    if ch_dict[key] != 0:\n",
    "        ch_dict_new[key] = ch_dict[key]\n",
    "\n",
    "x = np.arange(0, len(ch_dict_new))\n",
    "height = (np.array(list(ch_dict_new.values())) / n_epochs) * 100\n",
    "tick_label = list(ch_dict_new.keys())\n",
    "\n",
    "percent_rejected = (len(rejected_epochs) / n_epochs) * 100\n",
    "\n",
    "plt.bar(x, height, color = \"gray\", alpha = 0.5)\n",
    "plt.title(ID + \" Response Locked Drop Log,\" + \" Total Rejected = \" + str(np.round(percent_rejected, 2)) + \"%\")\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tick_label)\n",
    "ax.tick_params(axis = 'x', labelrotation = 45)\n",
    "ax.set_ylabel(\"% Rejected\")\n",
    "plt.grid(axis = 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add trial numbers of baseline epochs that were rejected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in baseline_rejected_epochs:\n",
    "    if i not in rejected_epochs:\n",
    "        rejected_epochs.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Rejected Epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response_locked_epochs.drop(np.array(rejected_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Offset Epochs\n",
    "- Go from end of encoding epoch onwards for a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_event_dict = {'End_Encode/600':101,'End_Encode/650':102,'End_Encode/700':103,\n",
    "                  'End_Encode/750':104,'End_Encode/800':105,'End_Encode/850':106,\n",
    "                  'End_Encode/900':107,'End_Encode/950':108,'End_Encode/1000':109}\n",
    "\n",
    "offset_epochs = mne.Epochs(raw, events, event_id = offset_event_dict, preload = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add RT related Metadata to the offset Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt=events[offset_epochs.selection+2]-events[offset_epochs.selection+1]\n",
    "rt=rt[:,0]*1000/512\n",
    "mean_rt=np.zeros(len(rt))\n",
    "median_rt=np.zeros(len(rt))\n",
    "\n",
    "gaps = events[offset_epochs.selection+1]-events[offset_epochs.selection]\n",
    "gaps=gaps[:,0]*1000/512\n",
    "\n",
    "RT_gap = rt + gaps\n",
    "\n",
    "for j in range(0,len(av_response_time)):\n",
    "    mean_rt[offset_epochs.events[:,2]==101+j]=av_response_time[j]+50\n",
    "    median_rt[offset_epochs.events[:,2]==101+j]=median_response_time[j]+50\n",
    "\n",
    "Over_Under=np.zeros(len(rt))\n",
    "Over_Under[rt>median_rt]=1\n",
    "Trigger=offset_epochs.events[:,2]\n",
    "offset_metadata={'trial_number':range(len(offset_epochs.events)),\n",
    "         'Response_Time':rt,\n",
    "         'Over_Under':Over_Under,\n",
    "         'Mean_RT':mean_rt,\n",
    "         'Median_RT':median_rt,\n",
    "         'Trigger':Trigger,\n",
    "         'Gap': gaps,\n",
    "         'Reject_Artif': np.zeros(len(offset_epochs.events)),\n",
    "         'RT_gap': RT_gap}\n",
    "\n",
    "offset_metadata=pd.DataFrame(offset_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Offset Epochs Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_epoch_events = offset_epochs.events \n",
    "event_id = offset_event_dict\n",
    "preload = False\n",
    "verbose = False \n",
    "metadata = offset_metadata \n",
    "detrend = None\n",
    "\n",
    "tmin = -0.1 \n",
    "tmax =  1 \n",
    "reject = None\n",
    "baseline = None \n",
    "   \n",
    "offset_epochs = mne.Epochs(raw, offset_epoch_events, event_id = offset_event_dict, tmin = tmin, tmax = tmax, \n",
    "                           reject = reject, preload = preload, detrend = detrend, baseline = baseline, \n",
    "                           verbose = verbose, metadata = metadata)\n",
    "\n",
    "offset_epochs.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_epochs_data = offset_epochs.get_data()\n",
    "offset_epochs_data_baselined = np.zeros(offset_epochs_data.shape)\n",
    "\n",
    "for i in range(0, n_epochs):\n",
    "    for j in range(0, n_channels):\n",
    "        baseline = baseline_period[i, j]\n",
    "        offset_epochs_data_baselined[i, j, :] = offset_epochs_data[i, j, :] - baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreate Epochs Object\n",
    "Have to do this again with new data if baselining to pre-interval period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = offset_epochs_data_baselined\n",
    "offset_epoch_events = offset_epochs.events \n",
    "event_id = offset_event_dict\n",
    "tmin = offset_epochs.tmin\n",
    "reject = None\n",
    "baseline = None\n",
    "info = offset_epochs.info\n",
    "\n",
    "offset_epochs = mne.EpochsArray(data = data, info = info, events = offset_epoch_events, event_id = event_id, \n",
    "                              tmin = tmin, reject = reject, baseline = baseline, metadata = metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Artifact Rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_epochs_data = offset_epochs.get_data()\n",
    "n_epochs, n_channels, n_times = offset_epochs_data.shape\n",
    "rejected_epochs = []\n",
    "\n",
    "ch_names = raw.ch_names\n",
    "ch_dict = {}\n",
    "\n",
    "for ch in ch_names[0:128]:\n",
    "    ch_dict[ch] = 0\n",
    "\n",
    "ch_dict[ch_names[-1]] = 0\n",
    "    \n",
    "ch_keys = list(ch_dict.keys())\n",
    "\n",
    "for i in range(0, n_epochs):\n",
    "    single_epoch_data = offset_epochs_data[i, :, :]\n",
    "    eeg_channels = single_epoch_data[0:128, :]\n",
    "    bipolar_channel = single_epoch_data[-1, :]\n",
    "\n",
    "    if np.sum((abs(eeg_channels) > 100e-6)) >= 1: # removing epochs with eeg channel amplitude greater than 100e-6\n",
    "        rejected_epochs.append(i)\n",
    "        \n",
    "    elif np.sum((abs(bipolar_channel) > 200e-6)) >= 1: # removing epochs with bipolar channel amplitude greater than 200e-6\n",
    "        ch_dict[ch_keys[-1]] += 1\n",
    "        if i not in rejected_epochs:\n",
    "            rejected_epochs.append(i)\n",
    "        \n",
    "    for j in range(0, 128):\n",
    "        single_channel_data = eeg_channels[j, :]\n",
    "        if np.sum(abs(single_channel_data) > 100e-6) >= 1:\n",
    "            ch_dict[ch_keys[j]] += 1\n",
    "\n",
    "offset_rejected_epochs = rejected_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot number of trials excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove channels with no epochs rejected ##\n",
    "ch_dict_new = {}\n",
    "\n",
    "for key in ch_dict.keys():\n",
    "    if ch_dict[key] != 0:\n",
    "        ch_dict_new[key] = ch_dict[key]\n",
    "\n",
    "x = np.arange(0, len(ch_dict_new))\n",
    "height = (np.array(list(ch_dict_new.values())) / n_epochs) * 100\n",
    "tick_label = list(ch_dict_new.keys())\n",
    "\n",
    "percent_rejected = (len(rejected_epochs) / n_epochs) * 100\n",
    "\n",
    "plt.bar(x, height, color = \"gray\", alpha = 0.5)\n",
    "plt.title(ID + \" Offset Drop Log,\" + \" Total Rejected = \" + str(np.round(percent_rejected, 2)) + \"%\")\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tick_label)\n",
    "ax.tick_params(axis = 'x', labelrotation = 45)\n",
    "ax.set_ylabel(\"% Rejected\")\n",
    "plt.grid(axis = 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add trial numbers of baseline epochs that were rejected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in baseline_rejected_epochs:\n",
    "    if i not in rejected_epochs:\n",
    "        rejected_epochs.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Rejected Epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "offset_epochs.drop(np.array(rejected_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update behavioural metadata to flag rejected trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_trials = encode_epochs.metadata[\"trial_number\"]\n",
    "repo_trials = repo_epochs.metadata[\"trial_number\"]\n",
    "response_locked_trials = response_locked_epochs.metadata[\"trial_number\"]\n",
    "offset_trials = offset_epochs.metadata[\"trial_number\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(encoding_metadata)):\n",
    "    trial_num = encoding_metadata.iat[i, 0]\n",
    "    if trial_num not in encode_trials:\n",
    "        encoding_metadata.iat[i, 7] = 1\n",
    "        \n",
    "for i in range(0, len(repo_metadata)):\n",
    "    trial_num = repo_metadata.iat[i, 0]\n",
    "    if trial_num not in repo_trials:\n",
    "        repo_metadata.iat[i, 7] = 1\n",
    "        \n",
    "for i in range(0, len(response_locked_metadata)):\n",
    "    trial_num = response_locked_metadata.iat[i, 0]\n",
    "    if trial_num not in response_locked_trials:\n",
    "        response_locked_metadata.iat[i, 7] = 1\n",
    "        \n",
    "for i in range(0, len(offset_metadata)):\n",
    "    trial_num = offset_metadata.iat[i, 0]\n",
    "    if trial_num not in offset_trials:\n",
    "        offset_metadata.iat[i, 7] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"Time_Reproduction_Data/\" + \"Alternate_Artifact_Rejection/\" + baseline_name + \"/Metadata/\" + str(ID) + \"_Encoding_Metadata_All.csv\"\n",
    "encoding_metadata.to_csv(fname, index = False)\n",
    "\n",
    "fname = \"Time_Reproduction_Data/\" + \"Alternate_Artifact_Rejection/\" + baseline_name + \"/Metadata/\" + str(ID) + \"_Repo_Metadata_All.csv\"\n",
    "repo_metadata.to_csv(fname, index = False)\n",
    "\n",
    "fname = \"Time_Reproduction_Data/\" + \"Alternate_Artifact_Rejection/\" + baseline_name + \"/Metadata/\" + str(ID) + \"_RL_Metadata_All.csv\"\n",
    "response_locked_metadata.to_csv(fname, index = False)\n",
    "\n",
    "fname = \"Time_Reproduction_Data/\" + \"Alternate_Artifact_Rejection/\" + baseline_name + \"/Metadata/\" + str(ID) + \"_Offset_Metadata_All.csv\"\n",
    "offset_metadata.to_csv(fname, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Epochs for Doing CSD Transformation in MATLAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving epochs in mat file form ##\n",
    "import scipy.io\n",
    "fname = \"Time_Reproduction_Data/\" + \"Alternate_Artifact_Rejection/\" + baseline_name + \"/Epochs_Arrays/\" + \"Encode/\" + str(ID) + \"_Encode_All.mat\"\n",
    "encode_epochs_array = encode_epochs.get_data()\n",
    "encode_epochs_array = np.moveaxis(encode_epochs_array, 0, -1)\n",
    "mat_dict = {\"encode_epochs_array\": encode_epochs_array}\n",
    "scipy.io.savemat(fname, mat_dict)\n",
    "\n",
    "## Also want to save the non-csd epochs for the metadata ##\n",
    "fname = \"Time_Reproduction_Data/\" + \"Alternate_Artifact_Rejection/\" + baseline_name + \"/Epochs/\" + \"Encode/\" + str(ID) + \"_Encode_All_epo.fif\"\n",
    "encode_epochs.save(fname = fname, overwrite = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduction Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "## Saving epochs in mat file form ##\n",
    "fname = \"Time_Reproduction_Data/\" + \"Alternate_Artifact_Rejection/\" + baseline_name + \"/Epochs_Arrays/\" + \"Reproduction/\" + str(ID) + \"_Reproduction_All.mat\"\n",
    "repo_epochs_array = repo_epochs.get_data()\n",
    "repo_epochs_array = np.moveaxis(repo_epochs_array, 0, -1)\n",
    "mat_dict = {\"repo_epochs_array\": repo_epochs_array}\n",
    "scipy.io.savemat(fname, mat_dict)\n",
    "\n",
    "## Also want to save non-csd epochs for metadata ##\n",
    "fname = \"Time_Reproduction_Data/\" + \"Alternate_Artifact_Rejection/\" + baseline_name + \"/Epochs/\" + \"Reproduction/\" + str(ID) + \"_Reproduction_All_epo.fif\"\n",
    "repo_epochs.save(fname = fname, overwrite = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response-Locked Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving epochs in mat file form ##\n",
    "fname = \"Time_Reproduction_Data/\" + \"Alternate_Artifact_Rejection/\" + baseline_name + \"/Epochs_Arrays/\" + \"Response_Locked/\" + str(ID) + \"_Response_Locked_All.mat\"\n",
    "response_locked_epochs_array = response_locked_epochs.get_data()\n",
    "response_locked_epochs_array = np.moveaxis(response_locked_epochs_array, 0, -1)\n",
    "mat_dict = {\"response_locked_epochs_array\": response_locked_epochs_array}\n",
    "scipy.io.savemat(fname, mat_dict)\n",
    "\n",
    "## Also want to save non-csd epochs for metadata ##\n",
    "fname = \"Time_Reproduction_Data/\" + \"Alternate_Artifact_Rejection/\" + baseline_name + \"/Epochs/\" + \"Response_Locked/\" + str(ID) + \"_RL_All_epo.fif\"\n",
    "response_locked_epochs.save(fname = fname, overwrite = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offset Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "## Saving epochs in mat file form ##\n",
    "fname = \"Time_Reproduction_Data/\" + \"Alternate_Artifact_Rejection/\" + baseline_name + \"/Epochs_Arrays/\" + \"Offset/\" + str(ID) + \"_Offset_All.mat\"\n",
    "offset_epochs_array = offset_epochs.get_data()\n",
    "offset_epochs_array = np.moveaxis(offset_epochs_array, 0, -1)\n",
    "mat_dict = {\"offset_epochs_array\": offset_epochs_array}\n",
    "savemat(fname, mat_dict)\n",
    "\n",
    "## Also want to save non-csd epochs for metadata ##\n",
    "fname = \"Time_Reproduction_Data/\" + \"Alternate_Artifact_Rejection/\" + baseline_name + \"/Epochs/\" + \"Offset/\" + str(ID) + \"_Offset_All_epo.fif\"\n",
    "offset_epochs.save(fname = fname, overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
